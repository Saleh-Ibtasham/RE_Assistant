{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48af5182-7d00-48a0-817d-f3d2ace93240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e28bcdc9-35a7-4dce-925c-6f66475f3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niloy/miniconda3/envs/pdf_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c6daa0-34c1-4500-a858-4a53d5838296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (FieldSchema, DataType, CollectionSchema, Collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "728974fa-1da1-4784-8d22-fd50595629cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections,MilvusClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707db5a9-01e3-4dc9-baf5-a0e11bd2e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryBufferMemory, CombinedMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.documents.base import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91edd1b1-87e2-4c44-8384-06210238411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# from transformers import AutoTokenizer, pipeline, AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from langchain_community.llms import CTransformers\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc7db97-78e1-4000-89fa-1973b90449a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69826670-ba85-4177-beba-f36fb5f00c9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loader = DirectoryLoader(\n",
    "#     os.path.abspath(\"/home/niloy/code/simple_chat/persist_storage\"),\n",
    "#     glob=\"**/*.pdf\",\n",
    "#     use_multithreading=True,\n",
    "#     show_progress=True,\n",
    "#     max_concurrency=50,\n",
    "#     loader_cls=UnstructuredPDFLoader,\n",
    "# )\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fec6b-7fda-46ab-bd6d-fb0ed5aa9bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# toc_regex = r\"\"\"\n",
    "#         ^(?:(?:Table of )?Contents|List of (?:Figures|Tables))?  # Optional Titles\n",
    "#         (?:\\s*\\n+)?                                             # Optional newline(s) \n",
    "#         (?:\\s*\\d+\\.?\\s+(.+)\\s*)?                                # Optional: number, period, title \n",
    "#         \\s*\\.{2,}\\s*\\d+                                         # Required dotted leader line and page number\n",
    "#     \"\"\"\n",
    "\n",
    "# pdf_contents = []\n",
    "# for page in tqdm(temp_data):\n",
    "#     page_data = page.page_content\n",
    "#     if (flags & flag_content_started) == flag_content_started:\n",
    "#         pdf_contents += page\n",
    "#     else:\n",
    "#         llm_answer = llm_tocqa_chain.invoke({\"context\": page_data})\n",
    "#         print(page_data)\n",
    "#         print(llm_answer[\"text\"])\n",
    "#         if llm_answer[\"text\"].lower().startswith(\"no\"):\n",
    "#             pdf_contents += page\n",
    "#         else:\n",
    "#             flags = flags | flag_toc_found\n",
    "#         if llm_answer[\"text\"].lower().startswith(\"no\") and (flags == flag_toc_found):\n",
    "#             flags = flags | flag_toc_ended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654d711c-eaa4-4430-9645-345d1e0736c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_contents(pdf_path):\n",
    "    toc_dotted_line_regex = r\".*\\.{2,}\\s*\\d+\"\n",
    "    pdf_name = os.path.basename(pdf_path)\n",
    "    print(\"saving .. \" + pdf_name)\n",
    "    original_doc = fitz.open(pdf_path)\n",
    "    output_doc = fitz.open()\n",
    "    pages_to_exclude = [] \n",
    "    flag_toc_found = 1\n",
    "    flag_toc_ended = 2\n",
    "    flag_content_started = 3\n",
    "    flags = 0\n",
    "    for i, page in enumerate(original_doc):\n",
    "        text = page.get_text()\n",
    "        if (flags & flag_content_started) == flag_content_started:\n",
    "            break\n",
    "        else:\n",
    "            if len(re.findall(toc_dotted_line_regex, text)) > 0:\n",
    "                pages_to_exclude.append(i)\n",
    "                flags = flags | flag_toc_found\n",
    "            elif (flags & flag_toc_found) == flag_toc_found and len(re.findall(toc_dotted_line_regex, text)) == 0:\n",
    "                flags = flags | flag_toc_ended\n",
    "    output_doc = original_doc\n",
    "    print(\"excluding pages ... \" + str(pages_to_exclude))\n",
    "    output_doc.delete_pages(from_page=1, to_page=pages_to_exclude[-1])\n",
    "    output_doc.save(pdf_name)\n",
    "\n",
    "    temp_loader = PyMuPDFLoader(pdf_name)\n",
    "    temp_data = temp_loader.load()\n",
    "    os.remove(pdf_name)\n",
    "    \n",
    "    return temp_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62f6a164-bf01-4cd0-8975-4844717ea008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_from_directory(directory_path):\n",
    "    pdfs = glob.glob(directory_path + \"/*.pdf\")\n",
    "    documents = []\n",
    "    for pdf in tqdm(pdfs):\n",
    "        documents.extend(extract_page_contents(pdf))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0aacb9c-a76f-4fcb-9269-2772dc6e5086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████████▊                                                           | 1/7 [00:00<00:00,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving .. 3EST000235-2562_Ben_CCU-S2_Safety_Manual.pdf\n",
      "excluding pages ... [1, 2]\n",
      "saving .. 3EST000235-2511_Ben_CCU-S2_ECP-USB_Content_Specification.pdf\n",
      "excluding pages ... [1, 2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████▌                                       | 3/7 [00:00<00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving .. 3EST000236-8112_Ben_CCU-S2_Product_Release Note_1.2.1.0.pdf\n",
      "excluding pages ... [1, 2, 3]\n",
      "saving .. 3EGM081750-0035_Gen_TBCI_and_TBI-ED_Interface_Control_Document.pdf\n",
      "excluding pages ... [1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████▍                             | 4/7 [00:00<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving .. 3EST000235-2556_Een_CCU-S2_User_Manual.pdf\n",
      "excluding pages ... [5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████████████████████████████▎                   | 5/7 [00:01<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving .. 3EGM007200D2806_Yen_MITRAC_Control_C&C_Platform_Catalogue.pdf\n",
      "excluding pages ... [1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 7/7 [00:02<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving .. 3EST000235-2552_Aen_CCU-S2_Interface_Control_Document.pdf\n",
      "excluding pages ... [1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents = load_from_directory(\"persist_storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5d5f18-53ed-4e9a-88ce-f397df16aabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61c6be72-ec47-42d1-9ed3-5384f3e83b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "docs = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9a334-4171-4583-8a6b-2d29c56e4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize torch settings\n",
    "DEVICE = torch.device('cuda:0' \n",
    "   if torch.cuda.is_available() \n",
    "   else 'cpu')\n",
    "# Load the encoder model from huggingface model hub.\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "\n",
    "\n",
    "# Get the model parameters and save for later.\n",
    "MAX_SEQ_LENGTH = encoder.get_max_seq_length() \n",
    "EMBEDDING_LENGTH = encoder.get_sentence_embedding_dimension()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a273339-5b5b-4ae9-bd6b-3df38514b4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 1344/1344 [01:04<00:00, 20.77it/s]\n"
     ]
    }
   ],
   "source": [
    "chunk_list = []\n",
    "for chunk in tqdm(docs):\n",
    "    # Generate embeddings using encoder from HuggingFace.\n",
    "    embeddings = torch.tensor(encoder.encode([chunk.page_content]))\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    converted_values = list(map(np.float32, embeddings))[0]\n",
    "    # Assemble embedding vector, original text chunk, metadata.\n",
    "    chunk_dict = {\n",
    "        'vector': converted_values,\n",
    "        'text': chunk.page_content,\n",
    "        'source': chunk.metadata['source'],}\n",
    "    chunk_list.append(chunk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd1fce6-8c95-4205-821b-ce7381ee0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e9189e-3d17-4641-9565-2fef15770e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT=f'http://localhost:19530'\n",
    "client = MilvusClient(\n",
    "  uri=ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e9f6b3-a3be-4680-be80-3be0781f40b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a69d7ba-7a55-4103-9d36-65d44919cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"MilvusDocs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71e7af84-79df-4c34-88a5-9442b398e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_collection(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "798b2a91-0417-46cf-a5ee-cdd57b56e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define a minimum expandable schema.\n",
    "fields = [\n",
    "   FieldSchema(\"pk\", DataType.INT64, is_primary=True, auto_id=True),\n",
    "   FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=EMBEDDING_LENGTH),]\n",
    "schema = CollectionSchema(\n",
    "   fields,\n",
    "   enable_dynamic_field=True,)\n",
    "\n",
    "index_params = client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\",\n",
    "    index_type=\"AUTOINDEX\",\n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "client.create_collection(collection_name=COLLECTION_NAME, schema=schema, index_params=index_params)\n",
    "\n",
    "# 2. Create the collection.\n",
    "# mc = Collection(\"MilvusDocs\", schema)\n",
    "\n",
    "# 3. Index the collection.\n",
    "# mc.create_index(\n",
    "#    field_name=\"vector\",\n",
    "#    index_params={\n",
    "#        \"index_type\": \"AUTOINDEX\",\n",
    "#        \"metric_type\": \"COSINE\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bbdc95f-cb31-4a35-8033-092e49512df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data into the Milvus collection.\n",
    "# insert_result = mc.insert(chunk_list)\n",
    "insert_result = client.insert(collection_name=COLLECTION_NAME, data=chunk_list)\n",
    "\n",
    "# After final entity is inserted, call flush \n",
    "# to stop growing segments left in memory.\n",
    "\n",
    "# print(mc.partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "050e43d8-a213-405e-aaa1-8e8ac80b0289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'collection_name': 'MilvusDocs',\n",
       " 'auto_id': True,\n",
       " 'num_shards': 1,\n",
       " 'description': '',\n",
       " 'fields': [{'field_id': 100,\n",
       "   'name': 'pk',\n",
       "   'description': '',\n",
       "   'type': <DataType.INT64: 5>,\n",
       "   'params': {},\n",
       "   'auto_id': True,\n",
       "   'is_primary': True},\n",
       "  {'field_id': 101,\n",
       "   'name': 'vector',\n",
       "   'description': '',\n",
       "   'type': <DataType.FLOAT_VECTOR: 101>,\n",
       "   'params': {'dim': 1024}}],\n",
       " 'aliases': [],\n",
       " 'collection_id': 449480778103619488,\n",
       " 'consistency_level': 2,\n",
       " 'properties': {},\n",
       " 'num_partitions': 1,\n",
       " 'enable_dynamic_field': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.describe_collection(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceae2555-0e53-4bf5-b486-6484acc97320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count(*)': 1344}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = client.query(collection_name=COLLECTION_NAME, output_fields=[\"count(*)\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9f7bbf1-2cd0-4036-8a57-a33764790225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.  Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "Previous Conversation:\n",
    "{chat_history}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summary_template = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "As a professional summarizer, create a concise and comprehensive summary of the provided context, while adhering to these guidelines:\n",
    "* Craft a summary that is detailed, thorough, in-depth, and complex, while maintaining clarity and conciseness.\n",
    "* Incorporate main ideas and essential information, eliminating extraneous language and focusing on critical aspects.\n",
    "* Rely strictly on the provided text, without including external information.\n",
    "* Format the summary in paragraph form for easy understanding.\n",
    "<</SYS>>\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "toc_qa_prompt = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "Here are characteristics of table of contents pages and main content pages:\n",
    "Table of Contents: Has numbered list, Lists chapter/section titles, lists tables/figures, has dots leading to numbers, often titled as \"table of contents\"/ \"list of figures\"/ \"list of tables\".\n",
    "Main Content: Contains names of sections and the description of those sections, paragraphs of text, likely longer.\n",
    "\n",
    "Considering these features, is the following page_content a table of contents page? Answer only in yes or no.\n",
    "<</SYS>>\n",
    "\n",
    "page_content: {context}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "summary_prompt = PromptTemplate.from_template(summary_template)\n",
    "toc_qa_prompt = PromptTemplate.from_template(toc_qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0adea803-280c-46e8-8544-f6a0a1af2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd10380-0f06-4856-bc2d-b57f7c0e0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d1dad6a-5394-46c9-9197-67f5b8fc0331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Quadro RTX 3000, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 24 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 24/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2773.58 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =     4.63 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     4.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 24  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 8  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "n_ctx = 4096\n",
    "\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_file=\"llama-2-7b-chat.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n",
    "\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"models/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    n_ctx=n_ctx,\n",
    "    n_gpu_layers = n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    # callback_manager=callback_manager,\n",
    "    # verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0648247f-9980-42b0-aa4b-0c10fc43286c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus search time: 0.0044934749603271484 sec\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"What are the complex type tables?\"\n",
    "QUERY = [QUESTION]\n",
    "\n",
    "# Before conducting a search, load the data into memory.\n",
    "client.load_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "# Embed the question using the same encoder.\n",
    "embedded_question = torch.tensor(encoder.encode([QUESTION]))\n",
    "# Normalize embeddings to unit length.\n",
    "embedded_question = F.normalize(embedded_question, p=2, dim=1)\n",
    "# Convert the embeddings to list of list of np.float32.\n",
    "embedded_question = list(map(np.float32, embedded_question))\n",
    "\n",
    "# Return top k results with AUTOINDEX.\n",
    "TOP_K = 5\n",
    "\n",
    "# Run semantic vector search using your query and the vector database.\n",
    "start_time = time.time()\n",
    "results = client.search(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    data=embedded_question, \n",
    "    anns_field=\"vector\", \n",
    "    output_fields=[\"text\", \"source\"], \n",
    "    limit=TOP_K)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Milvus search time: {elapsed_time} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5745ca1-63df-4bd4-9d33-6af6ce2f3216",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n, hits in enumerate(results):\n",
    "    print(f\"{n}th query result\")\n",
    "    for hit in hits:\n",
    "        print(hit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "045ea6a4-b84f-4929-a71a-904df166f056",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Assemble the context as a stuffed string.\n",
    "context = \"\"\n",
    "for r in results[0]:\n",
    "    text = r['entity']['text']\n",
    "    context += f\"{text} \"\n",
    "\n",
    "# Also save the context metadata to retrieve along with the answer.\n",
    "context_metadata = {\n",
    "   \"source\": results[0][0][\"entity\"][\"source\"],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90597b98-67b5-4aed-9079-138683e94597",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc4c49-690f-4203-83a1-f02153c02891",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "context_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d82ff0-6a4c-4f8d-819d-c6b5ca454d60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_question = prompt.format(context = context, question = question)\n",
    "full_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22764c44-34c2-4372-a7be-80abec353daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=1000, memory_key=\"chat_history\", input_key=\"question\")\n",
    "# context_memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit = 2000, memory_key = \"context\", input_key=\"question\")\n",
    "# memory = CombinedMemory(memories=[chat_memory, context_memory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a83f6b99-ba44-4300-8dde-67e5d3e04674",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_token_limit = 2000\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt, memory=chat_memory, verbose=True)\n",
    "llm_summary_chain = LLMChain(llm = llm, prompt=summary_prompt)\n",
    "llm_tocqa_chain = LLMChain(llm=llm, prompt=toc_qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c906d2-0e44-4c43-bf5f-475cd066519f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "llm_chain = prompt | llm\n",
    "llm_summary_chain = summary_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c377c92-3188-47da-ba52-b7a38a88ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token_count(context, encoding):\n",
    "    encoded_question = encoding.encode(context)\n",
    "    print(\"token count: \")\n",
    "    print(len(encoded_question) + 70)\n",
    "    return len(encoded_question) + 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65f23652-de53-44a3-8039-fe3dd3a6a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(results):\n",
    "    context = \"\"\n",
    "    for r in results[0]:\n",
    "        text = r['entity']['text']\n",
    "        context += f\"{text} \"\n",
    "    \n",
    "    # Also save the context metadata to retrieve along with the answer.\n",
    "    context_metadata = {\n",
    "       \"source\": results[0][0][\"entity\"][\"source\"],}\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a37b39d2-4b9e-42f5-acd0-bb116837f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(question, milvus_client, COLLECTION_NAME, encoder, context, encoding, prompt):\n",
    "    \n",
    "    milvus_client.load_collection(collection_name=COLLECTION_NAME)\n",
    "    # Embed the question using the same encoder.\n",
    "    embedded_question = torch.tensor(encoder.encode([question]))\n",
    "    # Normalize embeddings to unit length.\n",
    "    embedded_question = F.normalize(embedded_question, p=2, dim=1)\n",
    "    # Convert the embeddings to list of list of np.float32.\n",
    "    embedded_question = list(map(np.float32, embedded_question))\n",
    "    \n",
    "    # Return top k results with AUTOINDEX.\n",
    "    TOP_K = 5\n",
    "    \n",
    "    # Run semantic vector search using your query and the vector database.\n",
    "    start_time = time.time()\n",
    "    results = milvus_client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        data=embedded_question, \n",
    "        anns_field=\"vector\", \n",
    "        output_fields=[\"text\", \"source\"], \n",
    "        limit=TOP_K)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Milvus search time: {elapsed_time} sec\")\n",
    "    new_context = get_context(results)\n",
    "    temp_context = context + f\"{new_context}\"\n",
    "    # full_question = prompt.format(context = temp_context, question = question)\n",
    "    if check_token_count(temp_context, encoding) > summary_token_limit:\n",
    "        result = llm_summary_chain.invoke({\"context\": temp_context})\n",
    "        return result['text']\n",
    "    else:\n",
    "        return temp_context\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8068c9f-691a-449a-bc3d-995e6959a3ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is ccu s2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus search time: 0.01741504669189453 sec\n",
      "token count: \n",
      "1082\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.  Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "Previous Conversation:\n",
      "\n",
      "\n",
      "Context: CR no. \n",
      "Description \n",
      "NA \n",
      "This is the first release of the CCU-S2 System Software. Product name: \n",
      "CCU-S2 \n",
      " \n",
      "Version: \n",
      "1.2.0 \n",
      " \n",
      "Doc. type: User Manual \n",
      " \n",
      "Doc ID-number: 3EST000235-2556 \n",
      " \n",
      "Revision: _E \n",
      " \n",
      "Langauge: en  \n",
      "Page: 24/191 \n",
      " \n",
      " \n",
      " \n",
      "© ALSTOM SA 2022. All rights reserved. Reproduction, use or disclosure to third parties, without express written authorization from Alstom,  \n",
      "is strictly prohibited. \n",
      "3 Update factory installed unit \n",
      "The CCU-S2 is from factory equipped with a basic software installed that makes it possible to run a \n",
      "software update to the preferred CCU-S2 SYSW release. It does not contain a complete SYSW \n",
      "package, the SafeCPU image is not present since it will not be possible to start it without a safe \n",
      "application programmed on the device. The only device state it can enter is MAINTENANCE until it \n",
      "is updated. \n",
      "3.1 System Software package \n",
      "A full system software installation on a CCU-S2 is comprised of seven Images (DLUs). Four \n",
      "images are installed on the SafeCPU Module and three images are installed on the ComCPU \n",
      "Module. \n",
      "• \n",
      "SafeCPU Module comprises: Product name: \n",
      "CCU-S2 \n",
      " \n",
      "Version: \n",
      "1.2.0 \n",
      " \n",
      "Doc. type: User Manual \n",
      " \n",
      "Doc ID-number: 3EST000235-2556 \n",
      " \n",
      "Revision: _E \n",
      " \n",
      "Langauge: en  \n",
      "Page: 16/191 \n",
      " \n",
      " \n",
      " \n",
      "© ALSTOM SA 2022. All rights reserved. Reproduction, use or disclosure to third parties, without express written authorization from Alstom,  \n",
      "is strictly prohibited. \n",
      "2 Understanding the CCU-S2 \n",
      "2.1 Overview \n",
      "The CCU-S2 is the product name of the second-generation safety assessed (SIL2) train control \n",
      "computer system. It consists of the VCU-S2 hardware [25], the CCU-S2 system software [22] and \n",
      "the MITRAC PC Tools suite. The main purpose of the CCU-S2 is to provide a safety assessed \n",
      "program execution environment that enables implementation of safe train functions up to SIL2. All \n",
      "users of the CCU-S2 must follow the CCU-S2 Safety Manual [27] to be able to claim that \n",
      "implemented safety functions can use the safety foundation provided by the CCU-S2. \n",
      "For a more detailed overview of the CCU-S2, reference the CCU Application Guideline [26] and \n",
      "[28]. S2 hardware installation and commissioning. \n",
      "CCU-S2-SRAC-077 \n",
      " \n",
      "The CCU-S2 unit shall be placed in a compartment in order not to be ex-\n",
      "posed to conductive dust or water since it is rated only to meet IP20. See \n",
      "Hardware Component Description VCU-S2 [14] for more information. \n",
      "CCU-S2-SRAC-047 \n",
      " \n",
      "The instructions in the VCU-S2 Installation and Maintenance Instruction [11] \n",
      "must be followed to ensure safe installation of the product. When installing \n",
      "and commissioning safety related hardware, the information in the CCU-S2 \n",
      "Release Note [13] for the affected parts, any constraints or guidelines de-\n",
      "fined in the CCU-S2 User Manual [12] and Operating Environment in the \n",
      "Hardware Component Description VCU-S2 [14] shall also be followed. \n",
      "CCU-S2-SRAC-048 \n",
      " \n",
      "The instructions in the Shipping and Storage Instruction [15] must be fol-\n",
      "lowed to ensure safe transport and storage of the product. \n",
      "4.5.4 \n",
      "Operation and Maintenance Product name: \n",
      "CCU-S2 \n",
      " \n",
      "Version: \n",
      "1.2.0 \n",
      " \n",
      "Doc. type: User Manual \n",
      " \n",
      "Doc ID-number: 3EST000235-2556 \n",
      " \n",
      "Revision: _E \n",
      " \n",
      "Langauge: en  \n",
      "Page: 11/191 \n",
      " \n",
      " \n",
      " \n",
      "© ALSTOM SA 2022. All rights reserved. Reproduction, use or disclosure to third parties, without express written authorization from Alstom,  \n",
      "is strictly prohibited. \n",
      "1 Introduction \n",
      "1.1 Purpose \n",
      "This document describes the main functionalities of the CCU-S2 and how to use them.  \n",
      "This document is divided in the following main parts: \n",
      "Chapter \n",
      "Purpose \n",
      "2 \n",
      "An overview of the CCU-S2. \n",
      "3 \n",
      "How to update a factory installed CCU-S2 unit. \n",
      "4 \n",
      "Functional features \n",
      "5 \n",
      "Communication and I/O \n",
      "6 \n",
      "Diagnostics. \n",
      "7  \n",
      "Safety related verification activities required. \n",
      "8 \n",
      "Limitations and performance \n",
      "9 \n",
      "Troubleshooting the CCU-S2. \n",
      "10 - 11 \n",
      "Related devices and tools. \n",
      "12 \n",
      "Restrictions and Obligations \n",
      "13 \n",
      "Contact Information \n",
      "Table 1: Chapter overview \n",
      "1.2 Intended Audience \n",
      "\n",
      "Question: what is ccu s2?\n",
      "\n",
      "[/INST]\n",
      "\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\n",
    "chat_memory.clear()\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Chatbot: Thanks!\")\n",
    "        break\n",
    "    # context = \"\"\n",
    "    context = generate_context(user_input, client, COLLECTION_NAME, encoder, context, encoding, prompt)\n",
    "    result = llm_chain.invoke({\"context\": context, \"question\": user_input})\n",
    "    response = result['text']\n",
    "    print(\"Chatbot:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
